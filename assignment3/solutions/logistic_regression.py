# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nupMhjiIvZAscAvOIzF6b-CKGGIgMMYF
"""

# Commented out IPython magic to ensure Python compatibility.


import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
from collections import Counter
from sklearn import datasets
import cv2
# from google.colab.patches import cv2_imshow
import glob
from mpl_toolkits.mplot3d import Axes3D
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import sys

# data = np.array()
dir_path= '/home/ubuntu/Desktop/sem2/smai/assignment3/A3/A3'
def read_images(fimages_train):
    data = []
    # labels = []
    for i in range(len(fimages_train)):
        
        name_of_file = fimages_train[i]
        #print("name of file = ", name_of_file)
        splitname = name_of_file.split('.')
        name = str(dir_path + splitname[1]+'.'+splitname[2])
#         name = name[:-1]
        #print("fina name of file = ", name)
#         val = '/home/ubuntu/Desktop/sem2/smai/assignment3/A3/A3/dataset/001_000.jpg'

        gray_image = cv2.imread(name,0)

        #print("shape of color img ", gray_image)
        
        scale_percent = 60 # percent of original size
        width = int(gray_image.shape[1] * scale_percent / 100)
        height = int(gray_image.shape[0] * scale_percent / 100)
        dim = (width, height)
        # resize image
        resized = cv2.resize(gray_image, dim, interpolation = cv2.INTER_AREA)
        #print("shape of resized img = ", resized.shape)
        #print("shape of rayscaled img ", gray_image.shape)
        #print(type(gray_image))
        img1 = resized.flatten()
        #print("SHape of flattened img ", img1.shape)
        data.append(img1)
        # cv2_imshow(img)
        # cv2.waitKey(0)
        # cv2.destroyAllWindows()


    data = np.array(data)
    # labels = np.array(labels)
    #print("Ytpe of data = ",type(data),"Shape of data = ",data.shape," labels shape = ")
    return data

def sigmoid(X,weights):
    z = np.dot(X,weights)
    return (1/(1+np.exp(-z)))

def loss(h,y):
    val = (-y*np.log(h) - (1-y)*np.dot(1-h)).mean()
#     val = val.mean()
    return val

def gradient_descent(X, h ,y):
    val = np.dot(X.T,(h-y))
    val = val/y.shape[0]
    return val

def update_wts_loss(wt, lr, gradient):
    return (wt - lr*gradient)

def create_array(y_train, no, val):
    #print("array - ",no,val)
    arr = []
#     print("arr shape = ",arr.shape)
    for i in range(no):
        if y_train[i] == val:
            arr.append(1)
        else:
            arr.append(0)
    arr = np.asarray(arr)
    return arr

class LogisticRegression:
  
    alpha = 0.01
    iteration = 1000

    def train(self,X_train, y_train):
      arr_ones = np.ones((X_train.shape[0], 1)) 
      X = np.concatenate((arr_ones, X_train), axis=1)
      c_classes = np.unique(y_train)

      self.theta = np.ones(X.shape[1])
      for i in range(self.iteration):
          h = sigmoid(X,self.theta)
      #     print(h.shape)
          grad = gradient_descent(X, h, y_train)
      #     print(grad.shape)
          self.theta = update_wts_loss(self.theta , 0.001, grad)
      return self.theta

def computeDotProduct(X, u_reduce_):
    return(np.dot(X,u_reduce_))

class PCA:
    def fit(self,data, n_components):
        X = data
        X = (X-X.mean())/X.std()
        #print("shape of X = ",X.shape)
        self.U,S,V = np.linalg.svd(X.T)
        u_reduce_ = self.U[:, :n_components]
        new_data  = computeDotProduct(X, u_reduce_)
#         new_data  = np.dot(X,u_reduce_)
        
    
    def transform(self,X, n_components):
        u_reduce_ = self.U[:, :n_components]
        new_data  = computeDotProduct(X, u_reduce_)
        return new_data

class Q2:
    def fit(self, X, y, n_components):
    
        self.pca = PCA()
        self.pca.fit(X, n_components)
        X = self.pca.transform(X, n_components)
        c_classes = np.unique(y)
        self.theta_values = []
        for i in range(len(c_classes)):
            y_labels = create_array(y, X.shape[0], i)
            lr = LogisticRegression()
            theta = lr.train(X, y_labels)
            self.theta_values.append(theta)

        self.theta_values = np.array(self.theta_values)
        #print(self.theta_values.shape)


    def predict(self, X_test, n_components):
        values = []
        labels = []
        X_test = self.pca.transform(X_test, n_components)
        arr_ones = np.ones((X_test.shape[0], 1)) 
        X = np.concatenate((arr_ones, X_test), axis=1)
        values = []
        for i in range(self.theta_values.shape[0]):

            values.append(sigmoid(X, self.theta_values[i].T))
            #print(len(values))
        values = np.array(values)
        values = values.T
        for i in range(X_test.shape[0]):
            ind = np.argmax(values[i])
            #print("index = ", ind)
            labels.append(ind)

        return labels

# train_path = '/home/ubuntu/Desktop/sem2/smai/assignment3/A3/A3/sample_train.txt'
def perform_cleaning_for_train_data(train_path):
    images_path = []
    labels = []
    NameDict= {}
    with open (train_path, 'r') as file:
        while True:
            val = file.readline()
            #print(val)
            if val == '':
                break
            path,label = val.split()
            #print("path = ", path)
            #print(";abel = ",label)
            '''Put path in images_path to read from and labels in labels array'''
    #         labels.append(label)
            images_path.append(str(path))
            n_split = path.split('/')
            name = n_split[-1]
            name = name.split('.')[0].split('_')[0]
            #print("name = ",name)
            NameDict[int(name)] = label
            labels.append(int(name))
    #print(val)
    training_set = read_images(images_path)
    #print("training _ set = ", training_set.shape)
    #print(labels)
    #print(NameDict)
    return training_set, labels
    #     li = val.spl  name = fimages_train[i]


def perform_cleaning_for_test_data(file_path):
    images_path = []
    labels = []
#     NameDict= {}
    with open (file_path, 'r') as file:
        while True:
            val = file.readline()
            #print(val)
            if val == '':
                break
#             path,label = val.split()
#             print("path = ", path)
#             print(";abel = ",label)
            '''Put path in images_path to read from and labels in labels array'''
    #         labels.append(label)
            images_path.append(val)
            n_split = val.split('/')
            name = n_split[-1]
            name = name.split('.')[0].split('_')[0]
            #print("name = ",name)
#             NameDict[int(name)] = label
            labels.append(int(name))
    #print(val)
    training_set = read_images_for_test(images_path)
    #print("training _ set = ", training_set.shape)
    #print(labels)
    return training_set, labels

# dir_path= '/gdrive/My Drive/smai/assignment3/A3/A3'
def read_images_for_test(fimages_train):
    data = []
    # labels = []
    for i in range(len(fimages_train)):
        
        name_of_file = fimages_train[i]
        #print("name of file = ", name_of_file)
        splitname = name_of_file.split('.')
        name = str(dir_path + splitname[1]+'.'+splitname[2])
        name  = name[:-1]
        #print(name," ", type(name))
        
        #val = '/home/ubuntu/Desktop/sem2/smai/assignment3/A3/A3/dataset/005_012.jpg'
        #print(val," ", type(val))
#         raw = r"{}".format(name)
#         print("value of raw =", raw)
        gray_image = cv2.imread(name,0)

        #print("shape of color img ", gray_image)
        
        scale_percent = 60 # percent of original size
        width = int(gray_image.shape[1] * scale_percent / 100)
        height = int(gray_image.shape[0] * scale_percent / 100)
        dim = (width, height)
        # resize image
        resized = cv2.resize(gray_image, dim, interpolation = cv2.INTER_AREA)
        #print("shape of resized img = ", resized.shape)
        #print("shape of rayscaled img ", gray_image.shape)
        #print(type(gray_image))
        img1 = resized.flatten()
        #print("SHape of flattened img ", img1.shape)
        data.append(img1)
        # cv2_imshow(img)
        # cv2.waitKey(0)
        # cv2.destroyAllWindows()


    data = np.array(data)
    # labels = np.array(labels)
    #print("Ytpe of data = ",type(data),"Shape of data = ",data.shape," labels shape = ")
    return data

train_path = sys.argv[1]
train_data, train_labels = perform_cleaning_for_train_data(train_path)
#print(train_data)


test_path = sys.argv[2]
test_data,test_labels = perform_cleaning_for_test_data(test_path)
#print(test_data)

q2 = Q2()
q2.fit(train_data, train_labels,271)
preds = q2.predict(test_data, 271)
print(preds)

from sklearn.metrics import accuracy_score,  confusion_matrix
print(accuracy_score(test_labels, preds))
print(confusion_matrix(test_labels, preds))
